# @package _global_
model:
  name: SCIP

  propensity_treatment:                 # Missing hyperparameters are to be filled in command line / with tune_hparams = True / selected with +backbone/rmsn_hparams=...
    _target_: src.models.scip.SCIPPropensityNetworkTreatment
    seq_hidden_units:                   # hidden_size CDE
    dropout_rate:                       # Dropout of CDE hidden + output layers
    num_layer: 1                        # Number of CDE hidden layers
    inhomogeneous: True

    batch_size:
    max_grad_norm:
    optimizer:
      optimizer_cls: adam
      learning_rate:
      weight_decay: 0.0
      lr_scheduler: False

    tune_hparams: False                 # Hparam tuning
    tune_range: 50
    resources_per_trial:
    hparams_grid:

  propensity_history:                   # Missing hyperparameters are to be filled in command line / with tune_hparams = True / selected with +backbone/rmsn_hparams=...
    _target_: src.models.scip.SCIPPropensityNetworkHistory
    seq_hidden_units:                   # hidden_size CDE
    dropout_rate:                       # Dropout of CDE hidden + output layers
    num_layer: 1                        # Number of CDE hidden layers
    inhomogeneous: True

    batch_size:
    max_grad_norm:
    optimizer:
      optimizer_cls: adam
      learning_rate:
      weight_decay: 0.0
      lr_scheduler: False

    tune_hparams: False                 # Hparam tuning
    tune_range: 50
    resources_per_trial:
    hparams_grid:

  encoder:                              # Missing hyperparameters are to be filled in command line / with tune_hparams = True / selected with +backbone/rmsn_hparams=...
    _target_: src.models.scip.SCIPEncoder
    seq_hidden_units:                   # hidden_size CDE
    dropout_rate:                       # Dropout of CDE hidden + output layers
    num_layer: 1                        # Number of CDE hidden layers
    inhomogeneous: True

    batch_size:
    max_grad_norm:
    optimizer:
      optimizer_cls: adam
      learning_rate:
      weight_decay: 0.0
      lr_scheduler: False

    tune_hparams: False                   # Hparam tuning
    tune_range: 50
    resources_per_trial:
    hparams_grid:

  train_decoder: True
  decoder:                                # Missing hyperparameters are to be filled in command line / with tune_hparams = True / selected with +backbone/rmsn_hparams=...
    _target_: src.models.scip.SCIPDecoder
    seq_hidden_units:                     # hidden_size CDE
    dropout_rate:                         # Dropout of CDE hidden + output layers
    num_layer: 1                          # Number of CDE hidden layers
    inhomogeneous: True

    batch_size:
    max_grad_norm:
    optimizer:
      optimizer_cls: adam
      learning_rate:
      weight_decay: 0.0
      lr_scheduler: False

    tune_hparams: False                   # Hparam tuning
    tune_range: 20
    resources_per_trial:

exp:
  update_alpha: False
  stabilize: True